{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch transformers transformers accelerate gradio bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille"
      ],
      "metadata": {
        "id": "G8Ia88heUlwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a73e8f-67dc-4e84-bdbe-cc95984d1325"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.2/407.2 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "\n",
        "from tqdm import tqdm\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import List, Tuple, Optional\n",
        "from langchain.docstore.document import Document as LangchainDocument\n",
        "\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "index = faiss.read_index('knowledge_vector_1.index')\n",
        "with open('docs_processed.pkl', 'rb') as f:\n",
        "    docs_processed = pickle.load(f)\n",
        "\n",
        "lengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed, desc=\"Calculating token lengths\")]\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(docs_processed)})\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS(\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id={i: i for i in range(len(docs_processed))},\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "READER_MODEL_NAME = \"RJ1200/llama-3_3b-fine_tuned\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an AI assistant specializing in analyzing PDF documents. Your task is to generate a comprehensive question paper based on the provided PDF context.The question paper should include the following header:\n",
        "\n",
        "**Question Paper**\n",
        "\n",
        "Reg. No. ____________________\n",
        "\n",
        "End Semester Examination – Date\n",
        "\n",
        "Code: 18CS1004\n",
        "Duration: 3 hrs\n",
        "Sub. Name: PROGRAMMING FOR PROBLEM SOLVING\n",
        "Max. Marks: 100 .\n",
        " For each section mentioned, generate the exact number of questions as specified. Ensure that the questions are relevant, clear, and cover the key topics within the section. Reference specific page numbers or sections from the PDF whenever applicable. If the information needed to create questions is not available in the PDF context, clearly state that.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"PDF Context:\n",
        "        {context}\n",
        "        ---\n",
        "        For the following sections, generate the striclty required number of questions:\n",
        "        section_requirements\n",
        "    part A-10,\n",
        "    part B- 5,\n",
        "    part C- 4\n",
        "\n",
        "\n",
        "        ---\n",
        "        Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "        relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    context = \"\\nExtracted PDF content:\\n\" + \"\".join([f\"Section {str(i+1)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "question = \"generate end-sem question paper?\"\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
        "\n",
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n"
      ],
      "metadata": {
        "id": "WhqcHlSDWufB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7779792cbd9343ca978903450ec11905",
            "eb7be46376634c019620a506565f3478",
            "0785f46486974e849ce846d04c2f9347",
            "7f78dd2fcca9431d8b85370c4d0b83d4",
            "115d3460644447b5b15d903edffb4278",
            "6106b076994241009a04d4481f2ab6f6",
            "db11d3204f554c80a1c9173a85ec1ca7",
            "aba8de8a233d47338a4640f68d645539",
            "a124d2bea65b4ea3afca73ad8150b94f",
            "f3109d423fb04a6db7f64e7abc6ae2db",
            "3ec8e900cbdf44bba46f0460250054bd"
          ]
        },
        "outputId": "17511c36-9b55-4fa2-bcb7-e597ebaeab49"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Calculating token lengths: 100%|██████████| 45/45 [00:00<00:00, 1138.86it/s]\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7779792cbd9343ca978903450ec11905"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  5.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "Here is the question paper for PART A, with 10 questions.\n",
            "\n",
            "**Question Paper**\n",
            "\n",
            "Reg. No. ____________________\n",
            "\n",
            "End Semester Examination – April / May – 2019 \n",
            "\n",
            "Course Code           18CS1004 \n",
            "Course Name            PROGRAMMING FOR PROBLEM SOLVING \n",
            "Duration              3hrs \n",
            "Sub. Name             PROGRAMMING FOR PROBLEM SOLVING \n",
            "Max. Marks            100 \n",
            "\n",
            "**Part A (10 x 1 = 10 MARKS)**\n",
            "\n",
            "1. Define algorithm. CO1 1 \n",
            "2. Identify the invalid variable(s) in the following declarations. \n",
            "(a) int number;         (b) float for;        (c) invariable_count;          (d) int $main;  CO1 1 \n",
            "3. Predict the output of the following program. \n",
            "int a=10; \n",
            "int  *ptr=&a; \n",
            "printf(“%d”,*ptr); \n",
            "printf(“%d”, ++(*ptr));  CO6 U 1 \n",
            "4. __________ loop is called as exit  controlled loop.  CO3 1 \n",
            "5. State the string termination character. CO5 R 1 \n",
            "6. Predict the value at num[1][2] if \n",
            "int num[3][4]={1,3,4,2,4,5,6,7,8};  CO5 1 \n",
            "7. Define a function. CO4 1 \n",
            "8. Define function prototype. CO4 1 \n",
            "9. Predict the output of the following program. \n",
            "float a=10; \n",
            "printf(“%f”,a);  CO6 U 1 \n",
            "10. Predict the output for the following code. \n",
            "int a=10; \n",
            "int  *ptr=&a; \n",
            "printf(“%d”,*ptr); \n",
            "printf(“%d”,++(*ptr));  CO6 U 1 \n",
            "\n",
            "**Part B (6 x 3 = 18 MARKS)**\n",
            "\n",
            "11. Draw the block diagram of computer and describe its components. CO1 U 3 \n",
            "12. Compare and contrast ‘keyword’ and ‘identifier’ in C with examples. CO2 U 3 \n",
            "13. Write a simple program to find whether a number is odd or even. CO3 A 3 \n",
            "14. Write the algorithm for bubble sort program. CO4 U 3 \n",
            "15. Describe a function prototype along with its syntax. CO5 U 3 \n",
            "16. What is self-referential structure? Explain. CO6 A 3 \n",
            "17. Write a program in C to calculate the GCD of two numbers using recursion. CO5 6 \n",
            "18. List any three library functions for string processing in C with suitable examples. CO4 6 \n",
            "19. Explain the three types of error diagnostics. CO1 6 \n",
            "20. Define the steps to develop a program. CO1 6 \n",
            "21. Develop an application in C using structures to maintain and display the records with the name, register number, place and percentage of marks of n students. CO6 6 \n",
            "22. Develop a program to swap two values using pointers. CO5 6 \n",
            "23. Write a program in C to find the LCM of multiple numbers. CO5 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLWoHHYn0Wtk",
        "outputId": "31dbb523-f774-4644-cc98-8732d2f7037c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FPDF\n",
            "  Downloading fpdf-1.7.2.tar.gz (39 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: FPDF\n",
            "  Building wheel for FPDF (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FPDF: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=9cdee3622583c70b60d0cf4a72f301d9217338e78554c3691af959a2b198577c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/95/ba/f418094659025eb9611f17cbcaf2334236bf39a0c3453ea455\n",
            "Successfully built FPDF\n",
            "Installing collected packages: FPDF\n",
            "Successfully installed FPDF-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fpdf import FPDF\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# Initialize model components\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "index = faiss.read_index('knowledge_vector_1.index')\n",
        "with open('docs_processed.pkl', 'rb') as f:\n",
        "    docs_processed = pickle.load(f)\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(docs_processed)})\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS(\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id={i: i for i in range(len(docs_processed))},\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "READER_MODEL_NAME = \"RJ1200/llama-3_3b-fine_tuned\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Prompt template\n",
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an AI assistant specializing in analyzing PDF documents. Your task is to generate a comprehensive question paper based on the provided PDF context. The question paper should include the following header:\n",
        "        **Question Paper**\n",
        "        Reg. No. ____________________\n",
        "        End Semester Examination – Date\n",
        "        Code: 18CS1004\n",
        "        Duration: 3 hrs\n",
        "        Sub. Name: PROGRAMMING FOR PROBLEM SOLVING\n",
        "        Max. Marks: 100 .\n",
        "        For each section mentioned, generate the exact number of questions as specified. Ensure that the questions are relevant, clear, and cover the key topics within the section. Reference specific page numbers or sections from the PDF whenever applicable. If the information needed to create questions is not available in the PDF context, clearly state that.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"PDF Context:\n",
        "        {context}\n",
        "        ---\n",
        "        For the following sections, generate the required number of questions:\n",
        "        part A-10, part B- 5, part C- 4\n",
        "        ---\n",
        "        Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Function for RAG-based question paper generation\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "        relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    context = \"\\nExtracted PDF content:\\n\" + \"\".join([f\"Section {str(i+1)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "    final_prompt = tokenizer.apply_chat_template(prompt_in_chat_format, tokenize=False, add_generation_prompt=True).format(\n",
        "        question=question, context=context\n",
        "    )\n",
        "\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "# Generate PDF using FPDF\n",
        "def generate_pdf(text: str, filename: str = \"generated_question_paper.pdf\"):\n",
        "    # Replace unsupported characters with supported equivalents\n",
        "    text = text.replace('–', '-').replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', 16)\n",
        "    pdf.cell(200, 10, txt=\"Question Paper\", ln=True, align='C')\n",
        "\n",
        "    pdf.set_font(\"Arial\", '', 12)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Write each line in the PDF, wrapping as necessary\n",
        "    for line in text.split('\\n'):\n",
        "        pdf.multi_cell(0, 10, line)\n",
        "\n",
        "    pdf.output(filename)\n",
        "\n",
        "\n",
        "\n",
        "# Main code to generate question paper and PDF\n",
        "question = \"generate end-sem question paper?\"\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
        "\n",
        "# Print the answer\n",
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "\n",
        "# Generate the PDF\n",
        "generate_pdf(answer, \"end_sem_question_paper.pdf\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6d8c1786a38a45008fc6f82144537ba3",
            "98c05eda6d994adf86f1872593219261",
            "373b686c657546db8841c5c847234cb3",
            "4ce4d7f70e6c4fafb6251932b8d99eb6",
            "ffb8e901cb1d46a18c4334fd8dc3d095",
            "e4f6f361b3b94c0abf3f6fd03db65b9e",
            "7dd9f1f90886440c96fd58a04e347afd",
            "7a9cc20351e149ae9787217a6a16661a",
            "dab018dd55e0429cba8f3bb818f940f6",
            "ca5672f4e0e745c59244c475c50981a0",
            "06073c26fb6f466a8a3c710017b1bbf7"
          ]
        },
        "id": "2LIt-FKX0Q_4",
        "outputId": "04d0b8bd-9b70-4e37-ea2b-46ccdd5c61a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d8c1786a38a45008fc6f82144537ba3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================Answer==================================\n",
            "Here is the question paper for the course \"Programming for Problem Solving\" with the specified format:\n",
            "\n",
            "**Question Paper**\n",
            "\n",
            "**Reg. No.** _______________________________________\n",
            "**End Semester Examination – June / July 2024**\n",
            "**Course Code:** 18CS1004\n",
            "**Duration:** 3 hours\n",
            "**Course Name:** PROGRAMMING FOR PROBLEM SOLVING\n",
            "**Max. Marks:** 100\n",
            "\n",
            "**Part A (10 x 1 = 10 MARKS)**\n",
            "\n",
            "1. Define the concept of a stack. CO1 1\n",
            "2. Write a program in C to implement a stack using an array. CO2 1\n",
            "3. Predict the output of the following program: int a=10; int *ptr=&a; printf(\"%d\",*ptr); CO3 U 1\n",
            "4. ____________ is called as exit controlled loop. CO3 1\n",
            "5. List the two ways a string can be declared. CO4 R 1\n",
            "6. Identify the value of num[6] from the below line of code int num[]={9,6,4,2,3,5,1,7,8}; CO4 U 1\n",
            "7. State the string termination character. CO5 R 1\n",
            "8. Predict the output for the following code: int a=10; int *ptr=&a; printf(\"%d\",*ptr); printf(\"%d\",++(*ptr)); CO6 U 1\n",
            "9. Write a simple program to find whether a number is odd or even. CO3 A 3\n",
            "10. Write the algorithm for bubble sort program. CO4 U 3\n",
            "\n",
            "**Part B (6 x 3 = 18 MARKS)**\n",
            "\n",
            "1. Draw the block diagram of computer and describe its components. CO1 U 3\n",
            "2. Compare and contrast 'keyword' and 'identifier' in C with examples. CO2 U 3\n",
            "3. Write a simple program to find whether a number is prime or not. CO3 A 3\n",
            "4. Write the algorithm for merge sort program. CO4 U 3\n",
            "5. Describe a function prototype along with its syntax. CO5 U 3\n",
            "\n",
            "**Part C (6 x 12 = 72 MARKS)**\n",
            "\n",
            "(Answer any five questions from Q. No 17 to 23, Q. No 24 is Compulsory)\n",
            "\n",
            "17. a. Write the C program for bubble sort. Illustrate the various steps to sort the following data in ascending order. 10, 30, 5, 20, 9, 8 CO5 8\n",
            "b. Brief linear search with an example. CO5 4\n",
            "19. a. Write the C program for insertion sort. CO4 6\n",
            "b. List any three library functions for string processing in C with suitable examples. CO4 6\n",
            "20. a. Explain the three types of error diagnostics. CO1 6\n",
            "b. Define the steps to develop a program. CO1 6\n",
            "21. a. Write a program in C to add two matrices of MXN order. CO6 6\n",
            "b. List any three data types in C with their keyword and memory size. CO2 6\n",
            "22. a. Explain recursive functions in C with an example code. CO4 6\n",
            "b. Write a C program to find the maximum element in an integer array. CO5 6\n",
            "23. a. Discuss the main four components of a computer with suitable diagram. CO1 8\n",
            "b. Differentiate call by value and call by reference. CO1 4\n",
            "24. a. Develop an application in C using structures to maintain and display the records with the name, register number, place and percentage of marks of n students. CO6 6\n",
            "b. Develop a program to swap two values using pointers. CO5 6\n",
            "\n",
            "**Compulsory**\n",
            "\n",
            "24. a. Develop an application in C using structures to maintain and display the records with the name, register number, place and percentage of marks of n students. CO6 6\n",
            "b. Develop a program to swap two values using pointers. CO5 6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnicodeEncodeError",
          "evalue": "'latin-1' codec can't encode character '\\u2013' in position 436: ordinal not in range(256)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-dd3cb8100383>\u001b[0m in \u001b[0;36m<cell line: 136>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# Generate the PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mgenerate_pdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_sem_question_paper.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-dd3cb8100383>\u001b[0m in \u001b[0;36mgenerate_pdf\u001b[0;34m(text, filename)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;31m# Main code to generate question paper and PDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fpdf/fpdf.py\u001b[0m in \u001b[0;36moutput\u001b[0;34m(self, name, dest)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;31m#Finish document if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m         \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fpdf/fpdf.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m#close document\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enddoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fpdf/fpdf.py\u001b[0m in \u001b[0;36m_enddoc\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_enddoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_putheader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1636\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_putpages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_putresources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m         \u001b[0;31m#Info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fpdf/fpdf.py\u001b[0m in \u001b[0;36m_putpages\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m                 \u001b[0;31m# manage binary data as latin1 until PEP461 or similar is implemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1170\u001b[0;31m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"latin1\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mPY3K\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1171\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeEncodeError\u001b[0m: 'latin-1' codec can't encode character '\\u2013' in position 436: ordinal not in range(256)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from fpdf import FPDF\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "import faiss\n",
        "import pickle\n",
        "import torch\n",
        "from langchain.docstore import InMemoryDocstore\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\n",
        "from ragatouille import RAGPretrainedModel\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "# Initialize model components\n",
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
        "index = faiss.read_index('knowledge_vector_1.index')\n",
        "with open('docs_processed.pkl', 'rb') as f:\n",
        "    docs_processed = pickle.load(f)\n",
        "\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True}\n",
        ")\n",
        "\n",
        "docstore = InMemoryDocstore({i: doc for i, doc in enumerate(docs_processed)})\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS(\n",
        "    index=index,\n",
        "    docstore=docstore,\n",
        "    index_to_docstore_id={i: i for i in range(len(docs_processed))},\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "READER_MODEL_NAME = \"RJ1200/llama-3_3b-fine_tuned\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Prompt template\n",
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an AI assistant specializing in analyzing PDF documents. Your task is to generate a comprehensive question paper based on the provided PDF context. The question paper should include the following header:\n",
        "        **Question Paper**\n",
        "        Reg. No. ____________________\n",
        "        End Semester Examination – Date\n",
        "        Code: 18CS1004\n",
        "        Duration: 3 hrs\n",
        "        Sub. Name: PROGRAMMING FOR PROBLEM SOLVING\n",
        "        Max. Marks: 100 .\n",
        "        For each section mentioned, generate the exact number of questions as specified. Ensure that the questions are relevant, clear, and cover the key topics within the section. Reference specific page numbers or sections from the PDF whenever applicable. If the information needed to create questions is not available in the PDF context, clearly state that.\"\"\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"PDF Context:\n",
        "        {context}\n",
        "        ---\n",
        "        For the following sections, generate the required number of questions:\n",
        "        part A-10, part B- 5, part C- 4\n",
        "        ---\n",
        "        Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "# Function for RAG-based question paper generation\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "        relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    context = \"\\nExtracted PDF content:\\n\" + \"\".join([f\"Section {str(i+1)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "    final_prompt = tokenizer.apply_chat_template(prompt_in_chat_format, tokenize=False, add_generation_prompt=True).format(\n",
        "        question=question, context=context\n",
        "    )\n",
        "\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "# Generate PDF using FPDF\n",
        "def generate_pdf(text: str, filename: str = \"generated_question_paper.pdf\"):\n",
        "    # Replace unsupported characters with supported equivalents\n",
        "    text = text.replace('–', '-').replace('“', '\"').replace('”', '\"').replace('’', \"'\")\n",
        "\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "\n",
        "    pdf.set_font(\"Arial\", 'B', 16)\n",
        "    pdf.cell(200, 10, txt=\"Question Paper\", ln=True, align='C')\n",
        "\n",
        "    pdf.set_font(\"Arial\", '', 12)\n",
        "    pdf.ln(10)\n",
        "\n",
        "    # Write each line in the PDF, wrapping as necessary\n",
        "    for line in text.split('\\n'):\n",
        "        pdf.multi_cell(0, 10, line)\n",
        "\n",
        "    pdf.output(filename)\n",
        "\n",
        "# Gradio function to generate question paper and return PDF\n",
        "def gradio_generate_question_paper(question):\n",
        "    answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
        "    pdf_filename = \"end_sem_question_paper.pdf\"\n",
        "    generate_pdf(answer, pdf_filename)\n",
        "    return pdf_filename\n",
        "\n",
        "# Define Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# C Question Paper Generator\")\n",
        "\n",
        "    with gr.Row():\n",
        "        question_input = gr.Textbox(label=\"Enter your question (e.g., 'generate end-sem question paper')\")\n",
        "\n",
        "    generate_button = gr.Button(\"Generate Question Paper\")\n",
        "\n",
        "    pdf_output = gr.File(label=\"Download Question Paper\")\n",
        "\n",
        "    generate_button.click(gradio_generate_question_paper, inputs=question_input, outputs=pdf_output)\n",
        "\n",
        "# Launch Gradio app\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799,
          "referenced_widgets": [
            "c1a696dc8c884c328e558dabd7a1db4a",
            "73218dd1aa3c47a983f5ce3f2c1dd482",
            "ed12eeece9bf4c87a9b6c5a375ce2fde",
            "3a2a7adfef494bf5abdbc88a7f6a8b3b",
            "c6c5a0cdb99747ec95113be71ed84c9f",
            "fada0252003049c3ad2aa0b3a4bc5642",
            "902d4bec4c4843fdab3f5dd2b4dd8c9b",
            "1a5cedb1bb2b4db7a4bc96a34b64dcc7",
            "39fd889634ac431d90b31bbafcf351a7",
            "a6c8704d71fb49b685a50bb07ca98036",
            "1fc9da9222c043e0a5741512788c78c8"
          ]
        },
        "id": "_mnao_i_2Roc",
        "outputId": "998d6a77-9635-4db9-fd9f-8bab576b85c5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-ee38cbc102b2>:21: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(\n",
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1a696dc8c884c328e558dabd7a1db4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0abc032dcac22310eb.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0abc032dcac22310eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Assuming KNOWLEDGE_VECTOR_DATABASE is already created with your PDF content\n",
        "\n",
        "# Model initialization\n",
        "READER_MODEL_NAME = \"RJ1200/llama-3_3b-fine_tuned\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=1000,\n",
        ")\n",
        "\n",
        "# Initialize reranker\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"You are an AI assistant specializing in analyzing PDF documents. Your task is to generate a comprehensive question paper based on the provided PDF context.\n",
        "        For each section mentioned, generate the exact number of questions as specified.\n",
        "        Ensure that the questions are relevant, clear, and cover the key topics within the section.\n",
        "        Reference specific page numbers or sections from the PDF whenever applicable.\n",
        "        If the information needed to create questions is not available in the PDF context, clearly state that.\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"PDF Context:\n",
        "        {context}\n",
        "        ---\n",
        "        For the following sections, generate the required number of questions:\n",
        "        section_requirements\n",
        "    part A-10,\n",
        "    part B- 5,\n",
        "    part C- 4\n",
        "\n",
        "\n",
        "        ---\n",
        "        Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "        relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted PDF content:\\n\"\n",
        "    context += \"\".join([f\"Section {str(i+1)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "    # Generate an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "# Example usage\n",
        "question = \"generate end-sem question paper?\"\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)\n",
        "\n",
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3abfc0a35bd841878b5861280ab7af1f",
            "37210dc983654df09ffa79d932bd4bff",
            "378c447640044e9290df097a8b2d9476",
            "f84aa06325af486cb545034c9141cf3f",
            "d112aff9c74645cd8c65820232307e0b",
            "2b055648a5b44737a328910080b6a1f7",
            "2760eeed75e84277ac6b54bafba3fe86",
            "2420369c1b864888bd6a765ed52e666e",
            "d157e303eab847d5b8356e99294fb469",
            "a5a07631569c45e692386e9918768515",
            "c082d03b18084ea0887d0affe17e9678"
          ]
        },
        "id": "DFjCDxpOVsII",
        "outputId": "ad81bcdd-c4cd-4f56-b066-76b01d86902f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3abfc0a35bd841878b5861280ab7af1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Retrieving documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Reranking documents...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n",
            "100%|██████████| 1/1 [00:00<00:00,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=> Generating answer...\n",
            "==================================Answer==================================\n",
            "Here is the question paper for End-Semester exam in Programming for Problem Solving:\n",
            "\n",
            "**Part A (10 x 1 = 10 MARKS)**\n",
            "\n",
            "1. Define algorithm. CO1 1\n",
            "2. Identify the invalid variable(s) in the following declarations. CO1 1\n",
            "3. Predict the output of the following program. CO1 1\n",
            "4. Compare and contrast 'keyword' and 'identifier' in C with examples. CO2 1\n",
            "5. Write a simple program to find whether a number is odd or even. CO3 1\n",
            "6. Write the algorithm for bubble sort program. CO4 1\n",
            "7. Describe a function prototype along with its syntax. CO5 1\n",
            "8. What is self-referential structure? Explain. CO6 1\n",
            "9. Predict the output for the following code. CO6 1\n",
            "10. Write a recursive function to return the factorial of all the numbers in a one-dimensional array. CO5 1\n",
            "\n",
            "**Part B (6 x 3 = 18 MARKS)**\n",
            "\n",
            "1. Draw the block diagram of computer and describe its components. CO1 U 3\n",
            "2. Compare and contrast 'keyword' and 'identifier' in C with examples. CO2 U 3\n",
            "3. Write a simple program to find whether a number is odd or even. CO3 A 3\n",
            "4. Write the algorithm for bubble sort program. CO4 U 3\n",
            "5. Describe a function prototype along with its syntax. CO5 U 3\n",
            "6. What is self-referential structure? Explain. CO6 A 3\n",
            "\n",
            "**Part C (6 x 12 = 72 MARKS)**\n",
            "\n",
            "(Answer any five questions from Q. No 17 to 22, Q. No 23 is compulsory)\n",
            "\n",
            "17. Write the C program for bubble sort. Illustrate the various steps to sort the following data in ascending order. CO5 8\n",
            "18. Brief linear search with an example. CO5 4\n",
            "19. Differentiate call by value and call by reference with suitable examples. CO4 12\n",
            "20. Write a program in C to add two matrices of MXN order. CO6 6\n",
            "21. List any three library functions for string processing in C with suitable examples. CO4 6\n",
            "22. Explain the three types of error diagnostics. CO1 6\n",
            "23. Describe the steps to develop a program. CO1 6\n",
            "24. Develop an application in C using structures to maintain and display the records with the name, register number, place and percentage of marks of n students. CO6 6\n",
            "25. Develop a program to swap two values using pointers. CO5 6\n",
            "\n",
            "**Compulsory**\n",
            "\n",
            "26. Develop an application in C using structures to maintain and display the records with the name, register number, place and percentage of marks of n students. CO6 6\n",
            "27. Develop a program to swap two values using pointers. CO5 6\n",
            "==================================Source docs==================================\n",
            "Document 1------------------------------------------------------------\n",
            "Q. No. Questions Course  \n",
            "Outcome  Marks \n",
            "PART-A(10X1=10 MARKS) \n",
            "1. Give any two examples for secondary memory.  CO1 1 \n",
            "2. Predict the error in the following code.  \n",
            "    #include<stdio.h> \n",
            "    main(){ \n",
            "    int number; \n",
            "    printf(“Enter a number”); \n",
            "    scanf(“%d”,number); \n",
            "    printf(“the number entered is %d”, number); \n",
            "    } CO1 1 \n",
            "3. List any four keywords in C programming language.  CO2 1 \n",
            "4. Select the valid identifiers from the following. \n",
            "_num 2temp No_1 My  data \n",
            " CO2 1 \n",
            "5. Differentiate while and do -while control statements.  CO3 1 \n",
            "6. Give the syntax of goto statement.  CO3 1 \n",
            "7. Predict the value at num[1][2] if  \n",
            "int num[3][4]={1,3,4,2,4,5,6,7,8};  CO5 1 \n",
            "8. State the string termination character.  CO5 1 \n",
            "9. Define a function.  CO4 1 \n",
            "10. Define function prototype.  CO4 1 \n",
            " \n",
            "PART B (6 X 3= 18 MARKS)  \n",
            "11. Define any three characteristics of computer.  CO1 3 \n",
            "12. Illustrate any one logical operator in C with suitable example.  CO2 3 \n",
            "13. Differentiate selection control statement and iteration control statement.  CO3 3 \n",
            "14. Review the following integer array. \n",
            " int num[] = {5,6,7,8}; \n",
            "Give sample C code to di splay the elements of the num array. CO5 3 \n",
            "15. Define recursion.  CO4 3 \n",
            "16. Define self -referential structure.  CO5 3 \n",
            " \n",
            "PART C(6 X 12= 72 MARKS)\n",
            "Document 2------------------------------------------------------------\n",
            "19. a. Develop a C program to check whether a given number is Armstrong \n",
            "or not using while loop  structure.  CO3 6 \n",
            "b. Differentia te break and continue statements with suitable examples.  CO3 6 \n",
            "     \n",
            "20. a. Explain the  bubble sort algorithm with an example. CO5 6 \n",
            "b. Write a C program to find the sum of elements in an integer array. CO5 6 \n",
            "     \n",
            "21. a. Write a C program to generate Fibonacci series  using function . CO4 6 \n",
            "b. Explain recursive functions in C with an example code.  CO4 6 \n",
            "     \n",
            "22. a. Discuss the main four components of a computer with suitable \n",
            "diagram. CO1 8 \n",
            "b. Differentiate call by value and call by reference . CO1 4 \n",
            "     \n",
            "23. a. Discuss the various datatypes in C with their keyword and memory \n",
            "size.  CO2 6 \n",
            "b. Demonstrate the various string handling functions.  CO4 6 \n",
            "Compulsory:  \n",
            "24.  Develop an application in C using structures to maintain and display the n \n",
            "student records(record includes the student name, register number, place \n",
            "and percentage of marks).  CO5 12 \n",
            " \n",
            "     Reg.No. _____________ \n",
            " \n",
            "End Semester Examination – Nov / Dec – 2019 \n",
            "    \n",
            "Code           : 18CS1004 Duration      : 3hrs \n",
            "Sub. Name : PROGRAMMING FOR PROBLEM SOLVING Max. Marks : 100 \n",
            "Q. \n",
            "No. Questions Course \n",
            "Outcome  Marks \n",
            "PART – A (10 X 1 = 10 MARKS) \n",
            "1. Define algorithm.  CO1 1 \n",
            "2. Identify the invalid variable(s) in the following declations. \n",
            "(a) int number;         (b) float for;        (c) invariable_count;          (d) int $main;  CO1 1 \n",
            "3. Predict the output of the following program.\n",
            "Document 3------------------------------------------------------------\n",
            "b. Compare and contrast Structures with Arrays.  CO6 U 4 \n",
            "COMPULSORY QUESTION \n",
            "24. a. Write a recursive function to return the factorial of all the numbers in a \n",
            "one-dimensional array.  CO5 A 6 \n",
            " b. Write the C program for bubble sorting. Illustrate the various steps to \n",
            "sort the following data in ascending order.  \n",
            "10, 30, 5, 20, 9, 8  CO3 A 6 \n",
            " \n",
            " COURSE OUTCOMES \n",
            "CO1 Understand the fundamentals of computer and software development process.  \n",
            "CO2 Identify the data type to represent the real time data representation and operators for computation.  \n",
            "CO3 Prepare innovative solutions for the problem using branching and looping statements.  \n",
            "CO4 Decompose a problem into functions and synthesize a complete program using divide and conquer approach.  \n",
            "CO5 Formulate algorithms and programs using arrays, pointers, and structures.  \n",
            "CO6 Create a new application software to solve real -world problems.  \n",
            " \n",
            "Assessment Pattern as per Bloom’s Level \n",
            "CO / P Remember Understand Apply Analyze Evaluate Create Total \n",
            "CO1 2 15 - - - - 17 \n",
            "CO2 2 8 7 - - - 17 \n",
            "CO3 1 4 9 - - - 14 \n",
            "CO4 - 15 14 - - - 29 \n",
            "CO5 - 4 21 - - - 25 \n",
            "CO6 - 11 11 - - - 22 \n",
            " 124 \n",
            " \n",
            " \n",
            " \n",
            "END SEMESTER EXAMINATION – APRIL / MAY 2024 \n",
            " \n",
            "Course Code       18CS1004 Duration        3hrs \n",
            "Course Name      PROGRAMMING FOR PROBLEM SOLVING Max. Marks  100 \n",
            " \n",
            "Q. \n",
            "No. Questions  CO BL M \n",
            "PART – A (10 X 1 = 10 MARKS) \n",
            "1. State whether the following condition will evaluate to True or False. \n",
            "if ( (5<4) || (5>10) ) CO1 R 1 \n",
            "2. Locate the invalid variable(s) in the following declarations.\n",
            "Document 4------------------------------------------------------------\n",
            "iteration of a loop.  CO3 U 1 \n",
            "6. __________ loop is called as exit controlled loop. CO3 R 1 \n",
            "7. List the two ways a string can be declared. CO4 R 1 \n",
            "8. Identify the value of num[6] from the below line of code \n",
            "int num[]={9,6,4,2,3,5,1,7,8};  CO4 U 1 \n",
            "9. State the string termination character. CO5 R 1 \n",
            "10. Predict the output for the following code. \n",
            "int a=10; \n",
            "int  *ptr=&a; \n",
            "printf(“%d”,*ptr); \n",
            "printf(“%d”, ++(*ptr));  CO6 U 1 \n",
            "PART – B (6 X 3 = 18 MARKS) \n",
            "11. Draw the block diagram of computer and describe its components. CO1 U 3 \n",
            "12. Compare and contrast ‘keyword’ and ‘identifier’ in C with examples. CO2 U 3 \n",
            "13. Write a simple program to find whether a number is odd or even. CO3 A 3 \n",
            "14. Write the algorithm for bubble sort program. CO4 U 3 \n",
            "15. Describe a function prototype along with its syntax. CO5 U 3 \n",
            "16. What is self-referential structure? Explain. CO6 A 3 \n",
            "PART – C (6 X 12 = 72 MARKS) \n",
            "(Answer any five Questions from Q. No 17 to 23, Q. No 24 is Compulsory)\n",
            "Document 5------------------------------------------------------------\n",
            "20. a. Write the C program for bubble sort. Illustrate the various steps to \n",
            "sort the following data in ascending order.  \n",
            "10, 30, 5, 20, 9, 8  CO5 8 \n",
            "b. Brief linear search with an example. CO5 4 \n",
            "     \n",
            "21.  Differentiate call by value and call by reference with suitable \n",
            "examples.  CO4 12 \n",
            "     \n",
            "22. a. Write a program in C to add two matrices of MXN order. CO6 6 \n",
            "b. List any three library functions for string processing in C with \n",
            "suitable examples.  CO4 6 \n",
            "     \n",
            "23. a. Explain the three types of error diagnostics.  CO1 6 \n",
            "b. Define the steps to develop a program.  CO1 6 \n",
            "Compulsory:  \n",
            "24. a. Develop an application in C using structures to maintain and display \n",
            "the records with the name, register number, place and percentage of \n",
            "marks of n students.   CO6 6 \n",
            "b. Develop a program to swap two values using pointers.  CO5 6 \n",
            "  \n",
            "Reg.No. _____________ \n",
            " \n",
            "End Semester Examination –April / May - 2019 \n",
            "    \n",
            "Code           : 18CS1004 Duration      :  3hrs \n",
            "Sub. Name : PROGRAMMING FOR PROBLEM SOLVING Max. marks :  100 \n",
            " \n",
            "Q. \n",
            "No. Questions  \n",
            "Course  \n",
            "Outcome  Marks \n",
            "PART-A(10X1=10 MARKS) \n",
            "1. Expand SDLC.  CO1 1 \n",
            "2. Give the number of bytes reserved for a float variable . CO2 1 \n",
            "3. State whether the following condition will evaluate to True or False. \n",
            "if ( (3<4)&& (5>10) )  CO1 1 \n",
            "4. __________ loop is called as exit  controlled loop.  CO3 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8itz2uw9fSui"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3abfc0a35bd841878b5861280ab7af1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37210dc983654df09ffa79d932bd4bff",
              "IPY_MODEL_378c447640044e9290df097a8b2d9476",
              "IPY_MODEL_f84aa06325af486cb545034c9141cf3f"
            ],
            "layout": "IPY_MODEL_d112aff9c74645cd8c65820232307e0b"
          }
        },
        "37210dc983654df09ffa79d932bd4bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b055648a5b44737a328910080b6a1f7",
            "placeholder": "​",
            "style": "IPY_MODEL_2760eeed75e84277ac6b54bafba3fe86",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "378c447640044e9290df097a8b2d9476": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2420369c1b864888bd6a765ed52e666e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d157e303eab847d5b8356e99294fb469",
            "value": 2
          }
        },
        "f84aa06325af486cb545034c9141cf3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a07631569c45e692386e9918768515",
            "placeholder": "​",
            "style": "IPY_MODEL_c082d03b18084ea0887d0affe17e9678",
            "value": " 2/2 [00:27&lt;00:00, 12.77s/it]"
          }
        },
        "d112aff9c74645cd8c65820232307e0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b055648a5b44737a328910080b6a1f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2760eeed75e84277ac6b54bafba3fe86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2420369c1b864888bd6a765ed52e666e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d157e303eab847d5b8356e99294fb469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a5a07631569c45e692386e9918768515": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c082d03b18084ea0887d0affe17e9678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7779792cbd9343ca978903450ec11905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb7be46376634c019620a506565f3478",
              "IPY_MODEL_0785f46486974e849ce846d04c2f9347",
              "IPY_MODEL_7f78dd2fcca9431d8b85370c4d0b83d4"
            ],
            "layout": "IPY_MODEL_115d3460644447b5b15d903edffb4278"
          }
        },
        "eb7be46376634c019620a506565f3478": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6106b076994241009a04d4481f2ab6f6",
            "placeholder": "​",
            "style": "IPY_MODEL_db11d3204f554c80a1c9173a85ec1ca7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0785f46486974e849ce846d04c2f9347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aba8de8a233d47338a4640f68d645539",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a124d2bea65b4ea3afca73ad8150b94f",
            "value": 2
          }
        },
        "7f78dd2fcca9431d8b85370c4d0b83d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3109d423fb04a6db7f64e7abc6ae2db",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec8e900cbdf44bba46f0460250054bd",
            "value": " 2/2 [00:24&lt;00:00, 11.23s/it]"
          }
        },
        "115d3460644447b5b15d903edffb4278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6106b076994241009a04d4481f2ab6f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db11d3204f554c80a1c9173a85ec1ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aba8de8a233d47338a4640f68d645539": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a124d2bea65b4ea3afca73ad8150b94f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3109d423fb04a6db7f64e7abc6ae2db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec8e900cbdf44bba46f0460250054bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d8c1786a38a45008fc6f82144537ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98c05eda6d994adf86f1872593219261",
              "IPY_MODEL_373b686c657546db8841c5c847234cb3",
              "IPY_MODEL_4ce4d7f70e6c4fafb6251932b8d99eb6"
            ],
            "layout": "IPY_MODEL_ffb8e901cb1d46a18c4334fd8dc3d095"
          }
        },
        "98c05eda6d994adf86f1872593219261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f6f361b3b94c0abf3f6fd03db65b9e",
            "placeholder": "​",
            "style": "IPY_MODEL_7dd9f1f90886440c96fd58a04e347afd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "373b686c657546db8841c5c847234cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a9cc20351e149ae9787217a6a16661a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dab018dd55e0429cba8f3bb818f940f6",
            "value": 2
          }
        },
        "4ce4d7f70e6c4fafb6251932b8d99eb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca5672f4e0e745c59244c475c50981a0",
            "placeholder": "​",
            "style": "IPY_MODEL_06073c26fb6f466a8a3c710017b1bbf7",
            "value": " 2/2 [00:31&lt;00:00, 14.43s/it]"
          }
        },
        "ffb8e901cb1d46a18c4334fd8dc3d095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4f6f361b3b94c0abf3f6fd03db65b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dd9f1f90886440c96fd58a04e347afd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a9cc20351e149ae9787217a6a16661a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab018dd55e0429cba8f3bb818f940f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca5672f4e0e745c59244c475c50981a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06073c26fb6f466a8a3c710017b1bbf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c1a696dc8c884c328e558dabd7a1db4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73218dd1aa3c47a983f5ce3f2c1dd482",
              "IPY_MODEL_ed12eeece9bf4c87a9b6c5a375ce2fde",
              "IPY_MODEL_3a2a7adfef494bf5abdbc88a7f6a8b3b"
            ],
            "layout": "IPY_MODEL_c6c5a0cdb99747ec95113be71ed84c9f"
          }
        },
        "73218dd1aa3c47a983f5ce3f2c1dd482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fada0252003049c3ad2aa0b3a4bc5642",
            "placeholder": "​",
            "style": "IPY_MODEL_902d4bec4c4843fdab3f5dd2b4dd8c9b",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ed12eeece9bf4c87a9b6c5a375ce2fde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5cedb1bb2b4db7a4bc96a34b64dcc7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39fd889634ac431d90b31bbafcf351a7",
            "value": 2
          }
        },
        "3a2a7adfef494bf5abdbc88a7f6a8b3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6c8704d71fb49b685a50bb07ca98036",
            "placeholder": "​",
            "style": "IPY_MODEL_1fc9da9222c043e0a5741512788c78c8",
            "value": " 2/2 [00:30&lt;00:00, 14.02s/it]"
          }
        },
        "c6c5a0cdb99747ec95113be71ed84c9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fada0252003049c3ad2aa0b3a4bc5642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902d4bec4c4843fdab3f5dd2b4dd8c9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a5cedb1bb2b4db7a4bc96a34b64dcc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39fd889634ac431d90b31bbafcf351a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a6c8704d71fb49b685a50bb07ca98036": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fc9da9222c043e0a5741512788c78c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}